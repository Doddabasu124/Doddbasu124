{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Support ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependency libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,BatchNormalization\n",
    "import keras.backend as k\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Intent file\n",
    "with open('commands.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the WordnetLemmatizer\n",
    "stemm = WordNetLemmatizer()\n",
    "\n",
    "#get the punctuation in english grammar\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #If the required file(words,labels,documents) exist it will load the data \n",
    "    with open('file.pkl','rb') as f:\n",
    "        words,labels,documents = pickle.load(f)\n",
    "except:\n",
    "    #If not it will execute this except block of code and creates the pickel file\n",
    "    #Create empty lists of words, labels and documents\n",
    "    words = []\n",
    "    labels = []\n",
    "    documents = []\n",
    "    \n",
    "    for intent in data['intents']:\n",
    "        #loop through all patterns in intent file \n",
    "        for pattern in intent['patterns']:\n",
    "            \n",
    "            # apply string tokenization to each pattern\n",
    "            words_list = nltk.word_tokenize(pattern.lower())\n",
    "            #add each word into the words list\n",
    "            words.extend(words_list)\n",
    "            \n",
    "            #add each document with respect to its tag in corpus(documents)\n",
    "            documents.append((words_list,intent['tag']))\n",
    "            # Add only unique 'tag' into the Labels list\n",
    "            if intent['tag'] not in labels:\n",
    "                labels.append(intent['tag'])\n",
    "\n",
    "    #Stemming and removing dublicate words in words list\n",
    "    words = [stemm.lemmatize(w) for w in words if w not in punctuation]\n",
    "    words = sorted(set(words)) #it will remove dublicate words in words list\n",
    "    \n",
    "    labels = sorted(labels)\n",
    "\n",
    "    #Creating the serialized file for words and labels\n",
    "    with open('file.pkl','wb') as f:\n",
    "        pickle.dump((words,labels,documents),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output_empty = [0]*len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset which contain bag-of-words and associated label to it\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    \n",
    "    #get only words from each document\n",
    "    word_patterns = doc[0]\n",
    "    \n",
    "    #lemmatize the each word present in document\n",
    "    word_patterns = [stemm.lemmatize(word.lower()) for word in word_patterns if word not in punctuation]\n",
    "\n",
    "    #get the bag of words for each document in documents\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "    \n",
    "    output_raw = list(output_empty)\n",
    "    # add label index to the output raw based on each document\n",
    "    output_raw[labels.index(doc[1])] = 1\n",
    "    #append both bag of words and label to the training dataset  \n",
    "    training.append([bag,output_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the data\n",
    "random.shuffle(training)\n",
    "\n",
    "#Converting data into array type\n",
    "training = np.array(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dependent and independent features\n",
    "traning_x = list(training[:,0])\n",
    "traning_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating deep neural network model by using sequential API\n",
    "k.clear_session()\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(150,input_shape=(len(traning_x[0]),)  #adding 150 neurons of dense layer\n",
    "                ,activation='relu'))\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "model.add(Dense(100,activation='relu'))      #adding 100 neurons of dense layer\n",
    "model.add(BatchNormalization())          #apply batch normalization\n",
    "model.add(Dropout(0.30))               #apply dropout layer\n",
    "\n",
    "model.add(Dense(100,activation='relu'))      #adding 100 neurons of dense layer\n",
    "model.add(Dropout(0.30))                    #apply dropout layer\n",
    "\n",
    "model.add(Dense(len(traning_y[0]),activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 150)               13200     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 909       \n",
      "=================================================================\n",
      "Total params: 39,709\n",
      "Trainable params: 39,509\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Get the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer = 'adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0460 - accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0403 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0492 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0553 - accuracy: 0.9792\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0394 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0465 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0746 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0634 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0634 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0527 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0803 - accuracy: 0.9583\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0590 - accuracy: 0.9792\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 0.9792\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0456 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0447 - accuracy: 0.9792\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0710 - accuracy: 0.9792\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0961 - accuracy: 0.9792\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0548 - accuracy: 0.9792\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9792\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0481 - accuracy: 0.9792\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0456 - accuracy: 0.9792\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0421 - accuracy: 0.9792\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0432 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0260 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0504 - accuracy: 0.9792\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0424 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0209 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0315 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0345 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0328 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0348 - accuracy: 0.9792\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0487 - accuracy: 0.9792\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0370 - accuracy: 0.9792\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0410 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0330 - accuracy: 0.9792\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0186 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bf3c6988b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting data into model\n",
    "model.fit(traning_x,traning_y,epochs=100,batch_size=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save('chatbot_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the function to Tokenize and Lemmatize the user input/sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up (sentence):\n",
    "    #Tokenizing into words\n",
    "    sent_word = nltk.word_tokenize(sentence.lower())\n",
    "    \n",
    "    #Lemmatizing the each word\n",
    "    sent_word = [stemm.lemmatize(word) for word in sent_word if word not in punctuation]\n",
    "    return sent_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating function which generates bag of words by taking user input/sentence and words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sentence):\n",
    "    \n",
    "    #call the clean_function to Tokenize and lemmatize the user input\n",
    "    sentence_words = clean_up(sentence)\n",
    "    \n",
    "    #Creating the empty bag which has same length of words list\n",
    "    bag = [0]*len(words)\n",
    "    \n",
    "    #Performing one hot encoding to the bag\n",
    "    for w in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            #if user inputs(words) present in words list, then its going to add 1 into the bag based on index value\n",
    "            if word == w:\n",
    "                bag[i]=1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating function which returns prdicted label(or intent) and probability based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(sentence):\n",
    "    #call bag_of_words function to get the bag_of_words\n",
    "    bow = bag_of_words(sentence)\n",
    "    \n",
    "    #Predicting the label by taking bag_of_words\n",
    "    result = model.predict([bow])[0] \n",
    "    \n",
    "    #Assiging threshhold value to prevent the uncertain in model output\n",
    "    thr = 0.65\n",
    "    results = [[i,r] for i,r in enumerate(result) if r > thr]\n",
    "    \n",
    "    #Sort the results based on probability, which is predicted by model\n",
    "    results.sort(key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    #Convert numerical results into strings by looping through results\n",
    "      #creating empty list\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({'intent':labels[r[0]], 'probability':str(r[1])})\n",
    "        \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating response function to response back to user based highest probability of intent which is predicted by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(intents_list,data_intents_json):\n",
    "    #Creating tag which can have highest probability value\n",
    "    tag = intents_list[0]['intent']\n",
    "    \n",
    "    # get the intents from main data\n",
    "    list_of_intents = data['intents']\n",
    "    \n",
    "    #get the random response from responses list in main data based on highest probability of intent/tag\n",
    "    for i in list_of_intents:\n",
    "        if i['tag'] == tag:\n",
    "            #get the random response\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "            \n",
    "    return result,tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In final stage, we will feed the user’s input to the bot which it will response back to user based on input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: Im bot. I will answer your queries. If you want to exit, type Bye!\n",
      "user: hi\n",
      "BOT: Hello, thanks for asking\n",
      "user: what can u do\n",
      "BOT: Offering support for Adverse drug reaction, Blood pressure, Hospitals and Pharmacies\n",
      "user: hjbvf\n",
      "BOT: Please give me more info\n",
      "user: List pharmacies\n",
      "BOT: Please provide pharmacy name\n",
      "user: thanks\n",
      "BOT: Happy to help!\n",
      "user: Show blood pressure results for patient\n",
      "BOT: Patient ID?\n",
      "user: \n",
      "BOT: Please give me more info\n",
      "user: drugs module\n",
      "BOT: Navigating to Adverse drug reaction module\n",
      "user: thanks\n",
      "BOT: Happy to help!\n",
      "user: bye\n",
      "BOT: Bye! Come back again soon.\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"BOT: Im bot. I will answer your queries. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input('user: ').lower()\n",
    "    \n",
    "    #predicting labels by passing user_response to predict_labels function\n",
    "    ints=predict_labels(user_response)\n",
    "    \n",
    "    #if there is no input form user\n",
    "    if ints == []:\n",
    "        noanswer = [\"Sorry, can't understand you\", \"Please give me more info\", \"Not sure I understand\"]\n",
    "        print('BOT:',random.choice(noanswer))\n",
    "    \n",
    "    #Creating else block to replay back to user by calling response function\n",
    "    else:\n",
    "        #call the response function to get the response\n",
    "        res = response(ints,data)\n",
    "        \n",
    "        #if the predicted intent is other then 'goodbye', it will print the response\n",
    "        if res[1] != 'goodbye':\n",
    "            print('BOT:',res[0])\n",
    "            \n",
    "        else:\n",
    "            #Terminate the bot if user says bye\n",
    "            flag=False\n",
    "            print('BOT:',res[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
